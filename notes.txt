building-adaptive-rag/
├── src/                    # Source code
│   ├── workflow/           # Core workflow logic
│   │   ├── chains/         # LLM processing chains
│   │   │   ├── answer_grader.py
│   │   │   ├── generation.py
│   │   │   ├── hallucination_grader.py
│   │   │   ├── retrieval_grader.py
│   │   │   └── router.py
│   │   ├── nodes/        # Workflow nodes
│   │   │   ├── generate.py
│   │   │   ├── grade_documents.py
│   │   │   ├── retrieve.py
│   │   │   └── web_search.py
│   │   ├── consts.py     # Node constants
│   │   ├── graph.py      # Main workflow orchestration
│   │   └── state.py      # State management
│   ├── cli/              # Command line interface
│   │   └── main.py       # Interactive CLI
│   └── models/           # Model configurations
│       └── model.py      # LLM and embedding models
├── data/                 # Data processing
│   └── ingestion.py      # Document ingestion and vector store
├── assets/               # Static files and images
│   ├── LangChain-logo.png
│   └── Langgraph Adaptive Rag.png
├── tests/                # Test files
│   ├── __init__.py
│   └── test_chains.py    # Chain testing suite
├── .env                  # Environment variables
├── .gitignore
├── main.py              # Application entry point
├── README.md
└── requirements.txt


INTRODUCTION

Retrieval-Augmented Generation (RAG) has revolutionized how we build AI systems that can access and reason over external knowledge. 
However, as applications have grown more sophisticated, the limitations of traditional RAG approaches have become apparent. 
Today, we’re witnessing an evolution from simple, linear RAG pipelines to intelligent, adaptive systems that can dynamically adjust 
their retrieval and generation strategies based on query complexity and context.

2. Adaptive RAG
Adaptive RAG is an advanced strategy for RAG that intelligently combines 
(1) dynamic query analysis with 
(2) active/self-corrective mechanisms.

Adaptive RAG represents the most sophisticated evolution, addressing a fundamental insight: not all queries are created equal. 
The research reveals that real-world queries exhibit vastly different complexity levels:

- Simple queries: “Paris is the capital of what?” — Can be answered directly by LLMs
- Multi-hop queries: “When did the people who captured Malakoff come to the region where Philipsburg is located?” — Requires four reasoning steps
Press enter or click to view image in full size


1. Query Routing & Classification

The system begins with a trained complexity classifier that analyzes the incoming question. This isn’t just simple keyword matching; 
it’s a sophisticated assessment that determines:

Whether the query needs retrieval at all (parametric knowledge sufficient)
If retrieval is needed, what level of complexity is required
The optimal strategy ranges from no-retrieval, single-step, to multi-hop approaches

2. Dynamic Knowledge Acquisition Strategy

Based on the complexity classification, the system intelligently routes between:

- Index-based retrieval: For queries answerable from the existing knowledge base
- Web search: For queries requiring fresh information or when local retrieval fails
- No retrieval: For queries answerable directly from the model’s parametric knowledge

3. Multi-stage Quality Assurance

The system implements a comprehensive evaluation at multiple decision points:

- Document Relevance Assessment: Uses confidence scoring to evaluate retrieval quality
- Hallucination Detection: Verifies generated answers are grounded in retrieved evidence
- Answer Quality Evaluation: Ensures responses adequately address the original question


STEP BY STEP 

1. Defining state 
This GraphState class acts as the central data structure that flows through every node in our graph workflow. 
The question field holds the user's input query, generation stores the LLM's response, 
web_search is a boolean flag that determines whether we need to search the web for additional information, and documents contains all the retrieved documents from both local and web sources.

By using TypedDict, we ensure type safety while maintaining the flexibility needed for our dynamic workflow.

2. constants 
For conisistency accross graph 

